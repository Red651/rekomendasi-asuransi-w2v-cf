{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# latih word2vec dengan gensim dan corpus wikipedia bahasa indonesia\n",
    "https://medium.com/@diekanugraha/membuat-model-word2vec-bahasa-indonesia-dari-wikipedia-menggunakan-gensim-e5745b98714d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\farha\\miniconda3\\Lib\\site-packages\\gensim\\utils.py:1333: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles processed\n",
      "20000 articles processed\n",
      "30000 articles processed\n",
      "40000 articles processed\n",
      "50000 articles processed\n",
      "60000 articles processed\n",
      "70000 articles processed\n",
      "80000 articles processed\n",
      "90000 articles processed\n",
      "100000 articles processed\n",
      "110000 articles processed\n",
      "120000 articles processed\n",
      "130000 articles processed\n",
      "140000 articles processed\n",
      "150000 articles processed\n",
      "160000 articles processed\n",
      "170000 articles processed\n",
      "180000 articles processed\n",
      "190000 articles processed\n",
      "200000 articles processed\n",
      "210000 articles processed\n",
      "220000 articles processed\n",
      "230000 articles processed\n",
      "240000 articles processed\n",
      "250000 articles processed\n",
      "260000 articles processed\n",
      "270000 articles processed\n",
      "280000 articles processed\n",
      "290000 articles processed\n",
      "300000 articles processed\n",
      "310000 articles processed\n",
      "320000 articles processed\n",
      "330000 articles processed\n",
      "340000 articles processed\n",
      "350000 articles processed\n",
      "360000 articles processed\n",
      "370000 articles processed\n",
      "380000 articles processed\n",
      "390000 articles processed\n",
      "400000 articles processed\n",
      "410000 articles processed\n",
      "420000 articles processed\n",
      "430000 articles processed\n",
      "440000 articles processed\n",
      "450000 articles processed\n",
      "460000 articles processed\n",
      "470000 articles processed\n",
      "480000 articles processed\n",
      "490000 articles processed\n",
      "500000 articles processed\n",
      "Processing complete. 500903 articles processed\n",
      "Elapsed time: 0:48:53.775316\n"
     ]
    }
   ],
   "source": [
    "# Konversi Corpus Wikipedia Menjadi Teks\n",
    "import io\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import gensim\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    print('Loading Wikipedia corpus...')\n",
    "    id_wiki = gensim.corpora.WikiCorpus('Corpus_wikipedia/idwiki-20231201-pages-articles-multistream.xml.bz2',  dictionary={}, lower=True)\n",
    "    article_count = 0\n",
    "    with io.open('Corpus_wikipedia/idwiki-20231201-pages-articles-multistream.txt', 'w', encoding='utf-8') as wiki_txt:\n",
    "        for text in id_wiki.get_texts():\n",
    "            wiki_txt.write(' '.join(text) + '\\n')\n",
    "            article_count += 1\n",
    "            if article_count % 10000 == 0:\n",
    "                print('{} articles processed'.format(article_count))\n",
    "    print('Processing complete. {} articles processed'.format(article_count))\n",
    "    finish_time = time.time()\n",
    "    print('Elapsed time: {}'.format(timedelta(seconds=finish_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec model...\n",
      "Training complete. Elapsed time: 0:37:54.195273\n"
     ]
    }
   ],
   "source": [
    "# training model word2vec\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    print('Training word2vec model...')\n",
    "    sentences = word2vec.LineSentence('Corpus_wikipedia/idwiki-20231201-pages-articles-multistream.txt')\n",
    "    model = word2vec.Word2Vec(sentences, vector_size=200 ,workers=multiprocessing.cpu_count()-1)\n",
    "    model.save('Model_wikipedia/idwiki_word2vec.model')\n",
    "    finish_time = time.time()\n",
    "    print('Training complete. Elapsed time: {}'.format(timedelta(seconds=finish_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi word2vec dengan t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model...\n",
      "Model loaded. Generating t-SNE visualization...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    print('Loading word2vec model...')\n",
    "    model = word2vec.Word2Vec.load('Model_wikipedia/idwiki_word2vec.model')\n",
    "    print('Model loaded. Generating t-SNE visualization...')\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.key_to_index:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    tokens = np.array(tokens)\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    finish_time = time.time()\n",
    "    print('Elapsed time: {}'.format(timedelta(seconds=finish_time - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ratu', 0.9368245601654053), ('permaisuri', 0.8864334225654602), ('penguasa', 0.8795064687728882), ('firaun', 0.8777711987495422), ('kerajaan', 0.8759269714355469), ('permaisurinya', 0.855896532535553), ('sultan', 0.8493581414222717), ('rakyatnya', 0.8453277945518494), ('bangsawan', 0.8364384174346924), ('hulubalang', 0.8353058695793152)]\n"
     ]
    }
   ],
   "source": [
    "# mencoba model word2vec denga analogi kata menggunakan metode most_similar_cosmul dari gensim\n",
    "# laki:raja â€” perempuan:?\n",
    "\n",
    "# Load the model\n",
    "model_test = word2vec.Word2Vec.load('Model_wikipedia/idwiki_word2vec.model')\n",
    "\n",
    "# Perform the analogy task\n",
    "result_analogi = model_test.wv.most_similar_cosmul(positive=['perempuan', 'raja'], negative=['laki'])\n",
    "\n",
    "# Print the result\n",
    "print(result_analogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('manfaatnya', 0.8674401640892029), ('kesegaran', 0.8583281636238098), ('keuntungan', 0.8545964360237122), ('konsumsi', 0.8523703217506409), ('bermanfaat', 0.8400729298591614), ('pakan', 0.8386765718460083), ('sumbangsih', 0.8291501998901367), ('kemanfaatan', 0.8284624814987183), ('nutrisi', 0.8277636170387268), ('rezeki', 0.8253624439239502)]\n"
     ]
    }
   ],
   "source": [
    "# test dengan kata jenis asuransi Allianz\n",
    "result_analogi = model_test.wv.most_similar_cosmul(positive=['manfaat', 'allianz'], negative=['prudential'])\n",
    "\n",
    "# Print the result\n",
    "print(result_analogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('reasuransi', 0.9945306181907654), ('takaful', 0.9732854962348938), ('tabungan', 0.9588580131530762), ('sukuk', 0.9221739768981934), ('anuitas', 0.9207339882850647), ('premi', 0.897469162940979), ('deposito', 0.8810760378837585), ('avrist', 0.8764405846595764), ('kpr', 0.8697476387023926), ('santunan', 0.8678401112556458)]\n"
     ]
    }
   ],
   "source": [
    "result_analogi = model_test.wv.most_similar_cosmul(positive=['asuransi', 'manulife'])\n",
    "\n",
    "# Print the result\n",
    "print(result_analogi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
